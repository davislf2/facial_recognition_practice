{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: When you start kernel, make sure it's Python [anaconda]. Then it will be consistent with the conda environment you have sourced.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following practice is referenced from\n",
    "https://hackmd.io/s/ry8-ecSYM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19.15.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dlib\n",
    "dlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found 1 face(s) in this photograph.\n",
      "The left_eye in this face has the following points: [(95, 55), (97, 52), (100, 52), (104, 53), (101, 54), (98, 55)]\n",
      "The left_eyebrow in this face has the following points: [(90, 53), (93, 50), (97, 47), (102, 46), (107, 47)]\n",
      "The top_lip in this face has the following points: [(103, 76), (107, 74), (111, 72), (114, 72), (116, 71), (120, 72), (124, 73), (123, 74), (117, 73), (114, 74), (112, 74), (105, 76)]\n",
      "The nose_bridge in this face has the following points: [(111, 50), (112, 54), (113, 57), (114, 61)]\n",
      "The bottom_lip in this face has the following points: [(124, 73), (121, 75), (118, 76), (115, 77), (113, 77), (108, 77), (103, 76), (105, 76), (112, 74), (115, 74), (117, 73), (123, 74)]\n",
      "The right_eye in this face has the following points: [(117, 52), (119, 49), (122, 49), (125, 51), (123, 51), (120, 51)]\n",
      "The right_eyebrow in this face has the following points: [(113, 46), (117, 44), (121, 44), (125, 45), (128, 47)]\n",
      "The chin in this face has the following points: [(81, 61), (83, 67), (84, 74), (87, 80), (90, 85), (96, 89), (102, 91), (110, 91), (117, 91), (123, 89), (128, 86), (132, 82), (134, 77), (134, 71), (134, 65), (133, 60), (132, 54)]\n",
      "The nose_tip in this face has the following points: [(108, 67), (111, 67), (114, 67), (116, 66), (118, 65)]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import face_recognition\n",
    "import argparse\n",
    "\n",
    "# # construct the argument parse and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-f\", \"--filename\", required=True, help = \"filename of image to \\\n",
    "# detect faces\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "# # Load the jpg file into a numpy array\n",
    "# image = face_recognition.load_image_file(args[\"filename\"])\n",
    "\n",
    "# Load the jpg file into a numpy array\n",
    "image = face_recognition.load_image_file(\"example1.jpg\")\n",
    "\n",
    "\n",
    "# Find all facial features in all the faces in the image\n",
    "face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "\n",
    "print(\"I found {} face(s) in this photograph.\".format(len(face_landmarks_list)))\n",
    "\n",
    "for face_landmarks in face_landmarks_list:\n",
    "\n",
    "    # Print the location of each facial feature in this image\n",
    "    for facial_feature in face_landmarks.keys():\n",
    "        print(\"The {} in this face has the following points: {}\".format(facial_feature, face_landmarks[facial_feature]))\n",
    "\n",
    "    # Let's trace out each facial feature in the image with a line!\n",
    "    pil_image = Image.fromarray(image)\n",
    "    d = ImageDraw.Draw(pil_image)\n",
    "\n",
    "    for facial_feature in face_landmarks.keys():\n",
    "        d.line(face_landmarks[facial_feature], width=5)\n",
    "\n",
    "    pil_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-1. Face Detector(HOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of your image: (1024, 683, 3)\n",
      "dets rectangles[[(295, 146) (370, 221)]]\n",
      "--- 1.5113019943237305 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import dlib\n",
    "import time\n",
    "\n",
    "# Add this only to compute the time spent.\n",
    "start_time = time.time()\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Load the jpg file into a numpy array\n",
    "\n",
    "image = dlib.load_rgb_image('example2.jpg')\n",
    "print('Size of your image: ' + str(image.shape))\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "dets = detector(image,1)\n",
    "print(\"dets\",dets)\n",
    "for i, d in enumerate(dets):\n",
    "\n",
    "    # You can access the actual face itself like this:\n",
    "        face_image = image[d.top():d.bottom(), d.left():d.right()]\n",
    "        pil_image = Image.fromarray(face_image)\n",
    "        pil_image.show()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-2. Face Detector(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of your image: (1024, 683, 3)\n",
      "dets mmod_rectangles[[(293, 148) (361, 216)]]\n",
      "--- 10.699554920196533 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# import dlib\n",
    "import time\n",
    "\n",
    "# Add this only to compute the time spent.\n",
    "start_time = time.time()\n",
    "\n",
    "# Change to trained model\n",
    "detector = dlib.cnn_face_detection_model_v1('mmod_human_face_detector.dat')\n",
    "\n",
    "# Load the jpg file into a numpy array\n",
    "\n",
    "image = dlib.load_rgb_image('example2.jpg')\n",
    "print('Size of your image: ' + str(image.shape))\n",
    "detector = dlib.cnn_face_detection_model_v1('mmod_human_face_detector.dat')\n",
    "\n",
    "dets = detector(image,1)\n",
    "print(\"dets\",dets)\n",
    "for i, d in enumerate(dets):\n",
    "\n",
    "    # You can access the actual face itself like this:\n",
    "        # Change to d.rect.top() ...\n",
    "        face_image = image[d.rect.top():d.rect.bottom(), d.rect.left():d.rect.right()]\n",
    "        pil_image = Image.fromarray(face_image)\n",
    "        pil_image.show()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Face Landmark Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install\n",
    "```sh\n",
    "pip install opencv-python\n",
    "pip install imutils\n",
    "```\n",
    "**Note**:\n",
    "cv & cv2: C & C++ API, not represent versions. Both of them are OpenCV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "p = \"shape_predictor_5_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the input image and convert it to grayscale\n",
    "image = cv2.imread(\"example1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resize the image to 1/2 size of original\n",
    "small = cv2.resize(image, (0,0), fx=0.5, fy=0.5) \n",
    "gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# detect faces in the grayscale image\n",
    "rects = detector(gray, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop over the face detections\n",
    "for (i, rect) in enumerate(rects):\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        # loop over the (x, y)-coordinates for the facial landmarks\n",
    "        # and draw them on the image\n",
    "        for (x, y) in shape:\n",
    "                cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Bug: This one doesn't work!\n",
    "# show the output image with the face detections + facial landmarks\n",
    "cv2.imshow(\"Output\", small)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Facial Recognition - web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5001/ (Press CTRL+C to quit)\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "### It doesn't work on Jupyter Notebook\n",
    "import face_recognition\n",
    "from flask import Flask, jsonify, request, redirect\n",
    "\n",
    "# You can change this to any folder on your system\n",
    "ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def upload_image():\n",
    "    # Check if a valid image file was uploaded\n",
    "    if request.method == 'POST':\n",
    "        if 'file' not in request.files:\n",
    "            return redirect(request.url)\n",
    "\n",
    "        file = request.files['file']\n",
    "\n",
    "        if file.filename == '':\n",
    "            return redirect(request.url)\n",
    "\n",
    "        if file and allowed_file(file.filename):\n",
    "            # The image file seems valid! Detect faces and return the result.\n",
    "            return detect_faces_in_image(file)\n",
    "\n",
    "    # If no valid image file was uploaded, show the file upload form:\n",
    "    return '''\n",
    "    <!doctype html>\n",
    "    <title>Is this a picture of Obama?</title>\n",
    "    <h1>Upload a picture and see if it's a picture of Obama!</h1>\n",
    "    <form method=\"POST\" enctype=\"multipart/form-data\">\n",
    "      <input type=\"file\" name=\"file\">\n",
    "      <input type=\"submit\" value=\"Upload\">\n",
    "    </form>\n",
    "    '''\n",
    "\n",
    "\n",
    "def detect_faces_in_image(file_stream):\n",
    "    # Pre-calculated face encoding of Obama generated with face_recognition.face_encodings(img)\n",
    "    known_face_encoding = [-0.09634063,  0.12095481, -0.00436332, -0.07643753,  0.0080383,\n",
    "                            0.01902981, -0.07184699, -0.09383309,  0.18518871, -0.09588896,\n",
    "                            0.23951106,  0.0986533 , -0.22114635, -0.1363683 ,  0.04405268,\n",
    "                            0.11574756, -0.19899382, -0.09597053, -0.11969153, -0.12277931,\n",
    "                            0.03416885, -0.00267565,  0.09203379,  0.04713435, -0.12731361,\n",
    "                           -0.35371891, -0.0503444 , -0.17841317, -0.00310897, -0.09844551,\n",
    "                           -0.06910533, -0.00503746, -0.18466514, -0.09851682,  0.02903969,\n",
    "                           -0.02174894,  0.02261871,  0.0032102 ,  0.20312519,  0.02999607,\n",
    "                           -0.11646006,  0.09432904,  0.02774341,  0.22102901,  0.26725179,\n",
    "                            0.06896867, -0.00490024, -0.09441824,  0.11115381, -0.22592428,\n",
    "                            0.06230862,  0.16559327,  0.06232892,  0.03458837,  0.09459756,\n",
    "                           -0.18777156,  0.00654241,  0.08582542, -0.13578284,  0.0150229 ,\n",
    "                            0.00670836, -0.08195844, -0.04346499,  0.03347827,  0.20310158,\n",
    "                            0.09987706, -0.12370517, -0.06683611,  0.12704916, -0.02160804,\n",
    "                            0.00984683,  0.00766284, -0.18980607, -0.19641446, -0.22800779,\n",
    "                            0.09010898,  0.39178532,  0.18818057, -0.20875394,  0.03097027,\n",
    "                           -0.21300618,  0.02532415,  0.07938635,  0.01000703, -0.07719778,\n",
    "                           -0.12651891, -0.04318593,  0.06219772,  0.09163868,  0.05039065,\n",
    "                           -0.04922386,  0.21839413, -0.02394437,  0.06173781,  0.0292527 ,\n",
    "                            0.06160797, -0.15553983, -0.02440624, -0.17509389, -0.0630486 ,\n",
    "                            0.01428208, -0.03637431,  0.03971229,  0.13983178, -0.23006812,\n",
    "                            0.04999552,  0.0108454 , -0.03970895,  0.02501768,  0.08157793,\n",
    "                           -0.03224047, -0.04502571,  0.0556995 , -0.24374914,  0.25514284,\n",
    "                            0.24795187,  0.04060191,  0.17597422,  0.07966681,  0.01920104,\n",
    "                           -0.01194376, -0.02300822, -0.17204897, -0.0596558 ,  0.05307484,\n",
    "                            0.07417042,  0.07126575,  0.00209804]\n",
    "\n",
    "    # Load the uploaded image file\n",
    "    img = face_recognition.load_image_file(file_stream)\n",
    "    # Get face encodings for any faces in the uploaded image\n",
    "    unknown_face_encodings = face_recognition.face_encodings(img)\n",
    "\n",
    "    face_found = False\n",
    "    is_obama = False\n",
    "\n",
    "    if len(unknown_face_encodings) > 0:\n",
    "        face_found = True\n",
    "        # See if the first face in the uploaded image matches the known face of Obama\n",
    "        match_results = face_recognition.compare_faces([known_face_encoding], unknown_face_encodings[0])\n",
    "        if match_results[0]:\n",
    "            is_obama = True\n",
    "\n",
    "    # Return the result as json\n",
    "    result = {\n",
    "        \"face_found_in_image\": face_found,\n",
    "        \"is_picture_of_obama\": is_obama\n",
    "    }\n",
    "    return jsonify(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=5001, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Facial Recognition - Compute your facial feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cb5a0173ca55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mface_rec_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dlib_face_recognition_resnet_model_v1.dat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mobama_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"obama.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# Set the detector and shape predictor functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frontal_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "from scipy.spatial.distance import euclidean\n",
    "import pickle\n",
    "\n",
    "# Set the model paths\n",
    "shape_predictor_path = 'shape_predictor_5_face_landmarks.dat'\n",
    "face_rec_model_path = 'dlib_face_recognition_resnet_model_v1.dat'\n",
    "\n",
    "obama_vectors = pickle.load( open( \"obama.p\", \"rb\" ) )\n",
    "# Set the detector and shape predictor functions\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "sp = dlib.shape_predictor(shape_predictor_path)\n",
    "facerec = dlib.face_recognition_model_v1(face_rec_model_path)\n",
    "\n",
    "# Load the image you want to recognize\n",
    "img = dlib.load_rgb_image('obama.jpg')\n",
    "\n",
    "dets = detector(img, 1)\n",
    "\n",
    "for k, d in enumerate(dets):\n",
    "        print(\"Detection {}: Left: {} Top: {} Right: {} Bottom: {}\".format(\n",
    "            k, d.left(), d.top(), d.right(), d.bottom()))\n",
    "        # Get the landmarks/parts for the face in box d.\n",
    "        shape = sp(img, d)\n",
    "        face_descriptor = facerec.compute_face_descriptor(img, shape)\n",
    "        #print(face_descriptor)\n",
    "        distance = euclidean(face_descriptor, obama_vectors)\n",
    "        print('Distance between pre-computed facial vectors and picture we give is: ' +str(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
